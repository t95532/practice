{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-09-01T09:11:56.992215Z",
     "iopub.status.busy": "2023-09-01T09:11:56.991466Z",
     "iopub.status.idle": "2023-09-01T09:11:57.008792Z",
     "shell.execute_reply": "2023-09-01T09:11:57.007763Z",
     "shell.execute_reply.started": "2023-09-01T09:11:56.992180Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T11:11:47.587601Z",
     "iopub.status.busy": "2023-09-01T11:11:47.587216Z",
     "iopub.status.idle": "2023-09-01T11:11:47.610372Z",
     "shell.execute_reply": "2023-09-01T11:11:47.609440Z",
     "shell.execute_reply.started": "2023-09-01T11:11:47.587569Z"
    }
   },
   "outputs": [],
   "source": [
    "lines = []\n",
    "with open('/kaggle/input/next-word-prediction/1661-0.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        lines.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T10:05:43.323279Z",
     "iopub.status.busy": "2023-09-01T10:05:43.322826Z",
     "iopub.status.idle": "2023-09-01T10:05:43.329401Z",
     "shell.execute_reply": "2023-09-01T10:05:43.328388Z",
     "shell.execute_reply.started": "2023-09-01T10:05:43.323243Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T11:12:20.838701Z",
     "iopub.status.busy": "2023-09-01T11:12:20.838325Z",
     "iopub.status.idle": "2023-09-01T11:12:22.273795Z",
     "shell.execute_reply": "2023-09-01T11:12:22.272684Z",
     "shell.execute_reply.started": "2023-09-01T11:12:20.838669Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example data preprocessing\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Sample text data\n",
    "text_data = lines\n",
    "\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "\n",
    "# Create word-to-index and index-to-word mappings\n",
    "word_to_index = tokenizer.word_index\n",
    "index_to_word = {index: word for word, index in word_to_index.items()}\n",
    "\n",
    "# Encode text data as sequences of integers\n",
    "sequences = tokenizer.texts_to_sequences(text_data)\n",
    "\n",
    "# Generate input-output pairs\n",
    "input_sequences = []\n",
    "output_sequences = []\n",
    "for sequence in sequences:\n",
    "    for i in range(1, len(sequence)):\n",
    "        input_seq = sequence[:i]\n",
    "        output_seq = sequence[i]\n",
    "        input_sequences.append(input_seq)\n",
    "        output_sequences.append(output_seq)\n",
    "\n",
    "# Pad sequences to a fixed length if needed\n",
    "max_sequence_length = max(len(seq) for seq in input_sequences)\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T11:12:28.120295Z",
     "iopub.status.busy": "2023-09-01T11:12:28.119217Z",
     "iopub.status.idle": "2023-09-01T11:12:28.662359Z",
     "shell.execute_reply": "2023-09-01T11:12:28.661351Z",
     "shell.execute_reply.started": "2023-09-01T11:12:28.120256Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(word_to_index) + 1, output_dim=100, input_length=max_sequence_length))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(len(word_to_index) + 1, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T11:12:29.635755Z",
     "iopub.status.busy": "2023-09-01T11:12:29.635159Z",
     "iopub.status.idle": "2023-09-01T11:12:29.650051Z",
     "shell.execute_reply": "2023-09-01T11:12:29.648775Z",
     "shell.execute_reply.started": "2023-09-01T11:12:29.635717Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T10:55:12.268729Z",
     "iopub.status.busy": "2023-09-01T10:55:12.268000Z",
     "iopub.status.idle": "2023-09-01T10:55:12.276152Z",
     "shell.execute_reply": "2023-09-01T10:55:12.275041Z",
     "shell.execute_reply.started": "2023-09-01T10:55:12.268692Z"
    }
   },
   "outputs": [],
   "source": [
    "input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T11:12:39.328749Z",
     "iopub.status.busy": "2023-09-01T11:12:39.328367Z",
     "iopub.status.idle": "2023-09-01T11:12:39.347181Z",
     "shell.execute_reply": "2023-09-01T11:12:39.346118Z",
     "shell.execute_reply.started": "2023-09-01T11:12:39.328718Z"
    }
   },
   "outputs": [],
   "source": [
    "output_sequences = np.array(output_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T11:38:21.282492Z",
     "iopub.status.busy": "2023-09-01T11:38:21.282073Z",
     "iopub.status.idle": "2023-09-01T11:46:43.268813Z",
     "shell.execute_reply": "2023-09-01T11:46:43.267593Z",
     "shell.execute_reply.started": "2023-09-01T11:38:21.282458Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assuming you have prepared your training data as input_sequences and output_sequences\n",
    "model.fit(input_sequences, output_sequences, epochs=150, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-01T11:49:21.039486Z",
     "iopub.status.busy": "2023-09-01T11:49:21.039066Z",
     "iopub.status.idle": "2023-09-01T11:49:27.925147Z",
     "shell.execute_reply": "2023-09-01T11:49:27.923947Z",
     "shell.execute_reply.started": "2023-09-01T11:49:21.039455Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate predictions for the next word\n",
    "input_text = str(input())\n",
    "input_sequence = tokenizer.texts_to_sequences([input_text])[0]\n",
    "input_sequence = pad_sequences([input_sequence], maxlen=max_sequence_length, padding='pre')\n",
    "\n",
    "predicted_word_id = np.argmax(model.predict(input_sequence))\n",
    "predicted_word = index_to_word.get(predicted_word_id)\n",
    "\n",
    "print(f\"Predicted next word: {predicted_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3] *",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
